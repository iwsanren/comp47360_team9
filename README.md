# COMP47360_Team9 - Manhattan My Wayâ€‹

This is a full-stack web application designed to help New Yorkers make smarter, greener, and less crowded mobility decisions.

## Team Members

| Name               | Role                |
| ------------------ | ------------------- |
| HSUAN-YU TAN       | Back-End Code Lead  |
| Zhaofang He        | Maintenance Lead    |
| Neasa NÃ­ FhÃ¡tharta | Data Lead           |
| Martynas Kapocius  | Coordination Lead   |
| Prakhar Dayal      | Front-End Code Lead |

## Tech Stack

- **Frontend**: Next.js (React framework), Leaflet / Mapbox, TypeScript
- **Backend API**: Next.js API Routes, Node.js
- **Machine Learning**: scikit-learn, XGBoost (Python microservice)
- **Routing Engine**: OSRM
- **Deployment**: Docker + GitLab CI/CD on Linux server
- **Infrastructure**: Docker Compose, nginx

## Project Structure

```
â”œâ”€â”€ webapp/                 # Next.js frontend application
â”‚   â”œâ”€â”€ src/               # Source code
â”‚   â”œâ”€â”€ public/            # Static assets
â”‚   â”œâ”€â”€ Dockerfile         # Docker configuration
â”‚   â””â”€â”€ package.json       # Dependencies
â”œâ”€â”€ ml/                    # Python ML API service
â”‚   â”œâ”€â”€ app.py            # Flask API server
â”‚   â”œâ”€â”€ requirements.txt  # Python dependencies
â”‚   â””â”€â”€ Dockerfile        # Docker configuration
â”œâ”€â”€ scripts/              # Deployment scripts
â”‚   â”œâ”€â”€ setup-server.sh   # Server initialization
â”‚   â””â”€â”€ test-config.sh    # Configuration testing
â”œâ”€â”€ docs/                 # Documentation
â”œâ”€â”€ docker-compose.yml    # Multi-service configuration
â””â”€â”€ .gitlab-ci.yml       # CI/CD pipeline
```

## How to Run the Project

### Local Development

#### Prerequisites
- Docker and Docker Compose
- Node.js 18+ (for local development)
- Python 3.9+ (for ML service development)

#### Quick Start with Docker
```bash
# Clone the repository
git clone https://csgitlab.ucd.ie/ZhaofangHe/comp47360_team9.git
cd comp47360_team9

# Create environment file
cp .env.example .env
# Edit .env with your API keys

# Start all services
docker-compose up --build

# Access the application
# Frontend: http://localhost:3000
# ML API: http://localhost:5000
```

#### Development Mode
```bash
# Frontend development
cd webapp
npm install
npm run dev

# ML service development
cd ml
pip install -r requirements.txt
python app.py
```

### Production Deployment

The project uses GitLab CI/CD for automated deployment to a Linux server.

#### Deployment URLs
- **Staging**: http://137.43.49.26:3030 (develop branch)
- **Production**: http://137.43.49.26:8080 (main branch)

#### Setup Deployment
1. Configure GitLab CI/CD variables (see `SETUP-NEXT-STEPS.md`)
2. Set up SSH keys for server access
3. Push to `develop` branch for staging deployment
4. Merge to `main` branch for production deployment

For detailed setup instructions, see:
- `SETUP-NEXT-STEPS.md` - Quick setup guide
- `docs/quick-setup-guide.md` - Detailed instructions
- `CONFIGURATION-SUMMARY.md` - Complete configuration overview

## Features

- **Interactive Map**: Leaflet/Mapbox integration for NYC navigation
- **Route Planning**: Multi-modal transportation options
- **Real-time Data**: Weather, air quality, bike availability
- **ML Predictions**: Crowd density and busyness forecasting
- **Responsive Design**: Mobile-first approach
TODO: Add instructions for setting up Python service (e.g., Flask, FastAPI)

OSRM Routing Engine
TODO: Add how to launch and integrate OSRM

## Git Branching Model

We use a simple Git branching strategy based on `main`, `develop`, and `feature/*`.

- `main`: Stable and deployable version.
- `develop`: Integration branch for all features.
- `feature/*`: One branch per feature (created from `develop`, merged back into `develop`).
- `hotfix/*`: For urgent fixes (from `main`, merged into `main` and `develop`).

See `docs/git_workflow.md` for full explanation and diagram.

# ML API Integration Guide

This document explains how to call the ML API in different environments.

## Development Environment

### â¤ **Option 1. Running Flask app directly**

If you are running the Flask app directly on your local machine, fetch the API using: http://127.0.0.1:5000/predict-all

### â¤ **Option 2. Running the entire project with Docker**

If you are running the project via `docker`, fetch the API using: http://localhost:5001/predict-all

> ğŸ’¡ **Note:**  
> Make sure the containerâ€™s port is mapped to your local `5001` port. Adjust accordingly

---

## ğŸŒ Production Environment

In production, always fetch: /api/ml/predict-all

This endpoint is **proxied by Nginx or the Next.js API route to the ML server**. You do not need to call the ML server directly.


## Project Structure
```
comp47360_team9/
â”œâ”€â”€ .next/                  # [ignored] Next.js build output
â”œâ”€â”€ node_modules/           # [ignored] Node.js dependencies
â”œâ”€â”€ osrm-data/              # [ignored] OSRM-generated routing files
â”œâ”€â”€ .env                    # [ignored] Environment variables (API keys, secrets)
â”œâ”€â”€ .gitignore              # Git ignore rules
â”œâ”€â”€ README.md               # Project overview and instructions

# App Frontend (Next.js + Leaflet/Mapbox)
â”œâ”€â”€ public/                 # Static assets (e.g. favicon, icons)
â”œâ”€â”€ pages/                  # Next.js page routes (index.js, about.js etc.)
â”œâ”€â”€ components/             # Reusable UI components
â”œâ”€â”€ styles/                 # CSS modules or global styles

# Node.js Backend
â”œâ”€â”€ api/                    # Custom backend logic (if using API routes or server)
â”‚   â”œâ”€â”€ index.js
â”‚   â””â”€â”€ route.js

# Python ML Microservice
â”œâ”€â”€ ml/                     # Lightweight Python-based microservice (Flask/FastAPI)
â”‚   â”œâ”€â”€ model.pkl           # Trained model
â”‚   â”œâ”€â”€ predictor.py        # Inference script or API
â”‚   â””â”€â”€ requirements.txt    # Python dependencies

# OSRM
â”œâ”€â”€ osrm/                   # OSRM setup scripts or Docker config
â”‚   â””â”€â”€ run_osrm.sh

# Documentation
â”œâ”€â”€ docs/                   # Collaboration docs and team process
â”‚   â”œâ”€â”€ git_workflow.md
â”‚   â””â”€â”€ architecture.md

# Deployment
â”œâ”€â”€ render.yaml             # Render deployment config (optional)
â””â”€â”€ Dockerfile              # for full containerized deployment
```


