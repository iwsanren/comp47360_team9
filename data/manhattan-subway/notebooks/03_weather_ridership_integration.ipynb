{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84e8ebcc",
   "metadata": {},
   "source": [
    "# 03 - Weather-Ridership Integration Pipeline\n",
    "*Integration of Processed Subway Ridership and Weather Data*\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "Integrate processed 2024 Manhattan subway ridership and weather data into a unified dataset for downstream feature engineering and predictive modeling.\n",
    "\n",
    "---\n",
    "\n",
    "## Input Data Sources\n",
    "\n",
    "- `data/processed/subway_data_cleaned.parquet`  \n",
    "  → Hourly ridership data (1,052,709 records, 121 stations)\n",
    "\n",
    "- `data/processed/weather_data_cleaned.parquet`  \n",
    "  → Hourly weather data (8,783 records, 13 features)\n",
    "\n",
    "---\n",
    "\n",
    "## Integration Goals\n",
    "\n",
    "- Time-based merge using `transit_timestamp`\n",
    "- Validate alignment across both datasets (timezones, hourly resolution)\n",
    "- Output clean integrated dataset for modeling\n",
    "- Prepare for exploratory analysis and feature development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19dcaa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather-Ridership Integration Pipeline\n",
      "============================================================\n",
      "Execution Time: 2025-07-28 19:47:04\n",
      "Objective: Merge cleaned subway and weather data into unified dataset\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Weather-Ridership Integration Pipeline\n",
    "# =========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for clean output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "plt.style.use(\"default\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Execution header\n",
    "print(\"Weather-Ridership Integration Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Execution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"Objective: Merge cleaned subway and weather data into unified dataset\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb63f8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking input files...\n",
      "  Ridership: subway_data_cleaned.parquet - FOUND\n",
      "  Weather:   weather_data_cleaned.parquet - FOUND\n",
      "  Output directory: C:\\Users\\neasa\\manhattan-subway\\data\\processed\\integration\n",
      "\n",
      "File check complete. Ready to begin integration.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Directory and File Configuration\n",
    "# =========================================\n",
    "\n",
    "# Base directory paths\n",
    "PROJECT_DIR = Path(\".\").resolve().parents[0]\n",
    "DATA_DIR = PROJECT_DIR / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "INTEGRATION_DIR = PROCESSED_DIR / \"integration\"\n",
    "INTEGRATION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# File paths\n",
    "RIDERSHIP_FILE = PROCESSED_DIR / \"subway_data_cleaned.parquet\"\n",
    "WEATHER_FILE = PROCESSED_DIR / \"weather_data_cleaned.parquet\"\n",
    "OUTPUT_FILE = INTEGRATION_DIR / \"weather_ridership_integrated_2024.parquet\"\n",
    "\n",
    "# File checks\n",
    "print(\"Checking input files...\")\n",
    "print(f\"  Ridership: {RIDERSHIP_FILE.name} - {'FOUND' if RIDERSHIP_FILE.exists() else 'NOT FOUND'}\")\n",
    "print(f\"  Weather:   {WEATHER_FILE.name} - {'FOUND' if WEATHER_FILE.exists() else 'NOT FOUND'}\")\n",
    "print(f\"  Output directory: {INTEGRATION_DIR.resolve()}\")\n",
    "\n",
    "# Validate required files\n",
    "if not RIDERSHIP_FILE.exists() or not WEATHER_FILE.exists():\n",
    "    raise FileNotFoundError(\"Required processed files not found. Please run the earlier cleaning notebooks.\")\n",
    "\n",
    "print(\"\\nFile check complete. Ready to begin integration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abffd1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INPUT DATA QUALITY REVIEW\n",
      "============================================================\n",
      "No quality assessment files found.\n",
      "Proceeding without formal quality check.\n",
      "\n",
      "Proceeding with integration...\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Step 1: Load and Validate Input Data Quality\n",
    "# =========================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INPUT DATA QUALITY REVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Quality assessment file paths\n",
    "SUBWAY_QUALITY_FILE = PROCESSED_DIR / \"subway_data_quality_assessment.json\"\n",
    "WEATHER_QUALITY_FILE = PROCESSED_DIR / \"weather_data_quality_assessment.json\"\n",
    "\n",
    "# Check for quality files\n",
    "has_quality = SUBWAY_QUALITY_FILE.exists() and WEATHER_QUALITY_FILE.exists()\n",
    "\n",
    "if has_quality:\n",
    "    # Load quality JSONs\n",
    "    with open(SUBWAY_QUALITY_FILE, \"r\") as f:\n",
    "        subway_quality = json.load(f)\n",
    "    with open(WEATHER_QUALITY_FILE, \"r\") as f:\n",
    "        weather_quality = json.load(f)\n",
    "\n",
    "    # Subway metrics\n",
    "    subway_score = subway_quality.get(\"quality_scores\", {}).get(\"Overall Score\", 0)\n",
    "    subway_records = subway_quality.get(\"dataset_info\", {}).get(\"total_records\", \"N/A\")\n",
    "    subway_stations = subway_quality.get(\"dataset_info\", {}).get(\"unique_stations\", \"N/A\")\n",
    "    subway_temporal_pct = subway_quality.get(\"quality_scores\", {}).get(\"Temporal Coverage\", 0)\n",
    "\n",
    "    # Weather metrics\n",
    "    weather_score = weather_quality.get(\"quality_metrics\", {}).get(\"overall_score\", 0)\n",
    "    weather_records = weather_quality.get(\"processing_summary\", {}).get(\"final_integration_records\", \"N/A\")\n",
    "    weather_pct = weather_quality.get(\"processing_summary\", {}).get(\"processing_success_rate\", 0)\n",
    "    weather_ready_flag = weather_quality.get(\"feature_summary\", {}).get(\"integration_ready\", False)\n",
    "\n",
    "    # Display summary\n",
    "    print(\"SUBWAY RIDERSHIP DATA QUALITY\")\n",
    "    print(f\"  Overall Score:         {subway_score:.1f}/100\")\n",
    "    print(f\"  Total Records:         {subway_records:,}\")\n",
    "    print(f\"  Unique Stations:       {subway_stations}\")\n",
    "    print(f\"  Temporal Coverage:     {subway_temporal_pct:.1f}%\")\n",
    "\n",
    "    print(\"\\nWEATHER DATA QUALITY\")\n",
    "    print(f\"  Overall Score:         {weather_score:.1f}/100\")\n",
    "    print(f\"  Final Records:         {weather_records:,}\")\n",
    "    print(f\"  Success Rate:          {weather_pct:.1f}%\")\n",
    "    print(f\"  Integration Ready:     {weather_ready_flag}\")\n",
    "\n",
    "    # Integration readiness logic\n",
    "    ridership_ready = subway_score >= 95\n",
    "    weather_ready = weather_score >= 95 and weather_ready_flag\n",
    "    integration_ready = ridership_ready and weather_ready\n",
    "\n",
    "    print(\"\\nINTEGRATION READINESS\")\n",
    "    print(f\"  Ridership Data Ready:  {'YES' if ridership_ready else 'NO'}\")\n",
    "    print(f\"  Weather Data Ready:    {'YES' if weather_ready else 'NO'}\")\n",
    "    print(f\"  Integration Status:    {'READY' if integration_ready else 'REVIEW NEEDED'}\")\n",
    "\n",
    "    if not integration_ready:\n",
    "        print(\"\\nWarning: One or both datasets fall below the preferred quality threshold.\")\n",
    "        print(\"Proceeding with integration, but results should be validated downstream.\")\n",
    "    else:\n",
    "        print(\"\\nDatasets meet quality standards for integration.\")\n",
    "\n",
    "else:\n",
    "    print(\"No quality assessment files found.\")\n",
    "    print(\"Proceeding without formal quality check.\")\n",
    "    integration_ready = True\n",
    "\n",
    "print(\"\\nProceeding with integration...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec27fddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING PROCESSED DATASETS\n",
      "============================================================\n",
      "Loading ridership data...\n",
      "+ Ridership records loaded:   1,052,709\n",
      "  • Unique stations:          121\n",
      "  • Date range:               2024-01-01 00:00:00 to 2024-12-31 23:00:00\n",
      "  • Features:                 11\n",
      "  • Memory usage:             177.2 MB\n",
      "\n",
      "Loading weather data...\n",
      "+ Weather records loaded:     8,783\n",
      "  • Date range:               2024-01-01 00:00:00 to 2024-12-31 23:00:00\n",
      "  • Features:                 13\n",
      "  • Memory usage:             1.8 MB\n",
      "\n",
      "RIDERSHIP DATA SAMPLE:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_complex_id</th>\n",
       "      <th>station_complex</th>\n",
       "      <th>transit_timestamp</th>\n",
       "      <th>ridership</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>is_cbd</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>5 Av/59 St (N,R,W)</td>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>1141</td>\n",
       "      <td>40.76</td>\n",
       "      <td>-73.97</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>5 Av/59 St (N,R,W)</td>\n",
       "      <td>2024-01-01 01:00:00</td>\n",
       "      <td>351</td>\n",
       "      <td>40.76</td>\n",
       "      <td>-73.97</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>5 Av/59 St (N,R,W)</td>\n",
       "      <td>2024-01-01 02:00:00</td>\n",
       "      <td>94</td>\n",
       "      <td>40.76</td>\n",
       "      <td>-73.97</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_complex_id     station_complex   transit_timestamp  ridership  \\\n",
       "0                   8  5 Av/59 St (N,R,W) 2024-01-01 00:00:00       1141   \n",
       "1                   8  5 Av/59 St (N,R,W) 2024-01-01 01:00:00        351   \n",
       "2                   8  5 Av/59 St (N,R,W) 2024-01-01 02:00:00         94   \n",
       "\n",
       "   latitude  longitude  is_cbd  hour  day_of_week  month        date  \n",
       "0     40.76     -73.97       1     0            0      1  2024-01-01  \n",
       "1     40.76     -73.97       1     1            0      1  2024-01-01  \n",
       "2     40.76     -73.97       1     2            0      1  2024-01-01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WEATHER DATA SAMPLE:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transit_timestamp</th>\n",
       "      <th>temp</th>\n",
       "      <th>feels_like</th>\n",
       "      <th>dew_point</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>pressure</th>\n",
       "      <th>visibility</th>\n",
       "      <th>rain_1h</th>\n",
       "      <th>snow_1h</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>clouds_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>5.75</td>\n",
       "      <td>2.53</td>\n",
       "      <td>-1.44</td>\n",
       "      <td>59.00</td>\n",
       "      <td>4.47</td>\n",
       "      <td>1016.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01 01:00:00</td>\n",
       "      <td>5.43</td>\n",
       "      <td>2.32</td>\n",
       "      <td>-1.51</td>\n",
       "      <td>60.00</td>\n",
       "      <td>4.12</td>\n",
       "      <td>1016.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 02:00:00</td>\n",
       "      <td>5.45</td>\n",
       "      <td>2.35</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>62.00</td>\n",
       "      <td>4.12</td>\n",
       "      <td>1016.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>overcast clouds</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    transit_timestamp  temp  feels_like  dew_point  humidity  wind_speed  \\\n",
       "0 2024-01-01 00:00:00  5.75        2.53      -1.44     59.00        4.47   \n",
       "1 2024-01-01 01:00:00  5.43        2.32      -1.51     60.00        4.12   \n",
       "2 2024-01-01 02:00:00  5.45        2.35      -1.10     62.00        4.12   \n",
       "\n",
       "   pressure  visibility  rain_1h  snow_1h weather_main weather_description  \\\n",
       "0   1016.00    10000.00     0.12      NaN         Rain          light rain   \n",
       "1   1016.00    10000.00     0.14      NaN         Rain          light rain   \n",
       "2   1016.00    10000.00      NaN      NaN       Clouds     overcast clouds   \n",
       "\n",
       "   clouds_all  \n",
       "0      100.00  \n",
       "1      100.00  \n",
       "2      100.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RIDERSHIP COLUMNS:\n",
      "  (11): ['station_complex_id', 'station_complex', 'transit_timestamp', 'ridership', 'latitude', 'longitude', 'is_cbd', 'hour', 'day_of_week', 'month', 'date']\n",
      "\n",
      "WEATHER COLUMNS:\n",
      "  (13): ['transit_timestamp', 'temp', 'feels_like', 'dew_point', 'humidity', 'wind_speed', 'pressure', 'visibility', 'rain_1h', 'snow_1h', 'weather_main', 'weather_description', 'clouds_all']\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Step 2: Load Processed Datasets\n",
    "# =========================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING PROCESSED DATASETS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Confirm file paths again (should be previously defined)\n",
    "if not RIDERSHIP_FILE.exists() or not WEATHER_FILE.exists():\n",
    "    raise FileNotFoundError(\"Processed input files not found. Please run prior notebooks.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Load Ridership Data\n",
    "# -------------------------------\n",
    "print(\"Loading ridership data...\")\n",
    "ridership_df = pd.read_parquet(RIDERSHIP_FILE)\n",
    "ridership_df['transit_timestamp'] = pd.to_datetime(ridership_df['transit_timestamp'])\n",
    "\n",
    "print(f\"+ Ridership records loaded:   {len(ridership_df):,}\")\n",
    "print(f\"  • Unique stations:          {ridership_df['station_complex_id'].nunique()}\")\n",
    "print(f\"  • Date range:               {ridership_df['transit_timestamp'].min()} to {ridership_df['transit_timestamp'].max()}\")\n",
    "print(f\"  • Features:                 {len(ridership_df.columns)}\")\n",
    "print(f\"  • Memory usage:             {ridership_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# -------------------------------\n",
    "# Load Weather Data\n",
    "# -------------------------------\n",
    "print(\"\\nLoading weather data...\")\n",
    "weather_df = pd.read_parquet(WEATHER_FILE)\n",
    "weather_df['transit_timestamp'] = pd.to_datetime(weather_df['transit_timestamp'])\n",
    "\n",
    "print(f\"+ Weather records loaded:     {len(weather_df):,}\")\n",
    "print(f\"  • Date range:               {weather_df['transit_timestamp'].min()} to {weather_df['transit_timestamp'].max()}\")\n",
    "print(f\"  • Features:                 {len(weather_df.columns)}\")\n",
    "print(f\"  • Memory usage:             {weather_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# -------------------------------\n",
    "# Preview Samples and Structure\n",
    "# -------------------------------\n",
    "print(\"\\nRIDERSHIP DATA SAMPLE:\")\n",
    "display(ridership_df.head(3))\n",
    "\n",
    "print(\"\\nWEATHER DATA SAMPLE:\")\n",
    "display(weather_df.head(3))\n",
    "\n",
    "print(\"\\nRIDERSHIP COLUMNS:\")\n",
    "print(f\"  ({len(ridership_df.columns)}): {list(ridership_df.columns)}\")\n",
    "\n",
    "print(\"\\nWEATHER COLUMNS:\")\n",
    "print(f\"  ({len(weather_df.columns)}): {list(weather_df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b38ec97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TIMEZONE ALIGNMENT AND TEMPORAL VALIDATION\n",
      "============================================================\n",
      "Ridership timezone: None\n",
      "Weather timezone:   None\n",
      "\n",
      "TEMPORAL OVERLAP ANALYSIS\n",
      "Ridership range: 2024-01-01 00:00:00 to 2024-12-31 23:00:00\n",
      "Weather range:   2024-01-01 00:00:00 to 2024-12-31 23:00:00\n",
      "Overlap period:  2024-01-01 00:00:00 to 2024-12-31 23:00:00\n",
      "\n",
      "Overlap confirmed: 8783 hours\n",
      "  • Ridership coverage: 100.0%\n",
      "  • Weather coverage:   100.0%\n",
      "Temporal alignment: EXCELLENT\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Step 3: Timezone Alignment and Temporal Validation\n",
    "# =========================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TIMEZONE ALIGNMENT AND TEMPORAL VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check timezone awareness\n",
    "ridership_tz = ridership_df['transit_timestamp'].dt.tz\n",
    "weather_tz = weather_df['transit_timestamp'].dt.tz\n",
    "\n",
    "print(f\"Ridership timezone: {ridership_tz}\")\n",
    "print(f\"Weather timezone:   {weather_tz}\")\n",
    "\n",
    "# Ensure both are timezone-naive for merging\n",
    "if ridership_tz is not None:\n",
    "    ridership_df['transit_timestamp'] = ridership_df['transit_timestamp'].dt.tz_localize(None)\n",
    "    print(\"→ Timezone removed from ridership timestamps\")\n",
    "\n",
    "if weather_tz is not None:\n",
    "    weather_df['transit_timestamp'] = weather_df['transit_timestamp'].dt.tz_localize(None)\n",
    "    print(\"→ Timezone removed from weather timestamps\")\n",
    "\n",
    "# Validate date range overlap\n",
    "print(\"\\nTEMPORAL OVERLAP ANALYSIS\")\n",
    "\n",
    "ridership_range = (\n",
    "    ridership_df['transit_timestamp'].min(),\n",
    "    ridership_df['transit_timestamp'].max()\n",
    ")\n",
    "weather_range = (\n",
    "    weather_df['transit_timestamp'].min(),\n",
    "    weather_df['transit_timestamp'].max()\n",
    ")\n",
    "\n",
    "overlap_start = max(ridership_range[0], weather_range[0])\n",
    "overlap_end = min(ridership_range[1], weather_range[1])\n",
    "\n",
    "print(f\"Ridership range: {ridership_range[0]} to {ridership_range[1]}\")\n",
    "print(f\"Weather range:   {weather_range[0]} to {weather_range[1]}\")\n",
    "print(f\"Overlap period:  {overlap_start} to {overlap_end}\")\n",
    "\n",
    "if overlap_start >= overlap_end:\n",
    "    raise ValueError(\"ERROR: No temporal overlap between ridership and weather datasets\")\n",
    "\n",
    "# Compute overlap coverage\n",
    "overlap_hours = (overlap_end - overlap_start).total_seconds() / 3600\n",
    "ridership_hours = (ridership_range[1] - ridership_range[0]).total_seconds() / 3600\n",
    "weather_hours = (weather_range[1] - weather_range[0]).total_seconds() / 3600\n",
    "\n",
    "ridership_coverage = (overlap_hours / ridership_hours) * 100\n",
    "weather_coverage = (overlap_hours / weather_hours) * 100\n",
    "\n",
    "print(f\"\\nOverlap confirmed: {overlap_hours:.0f} hours\")\n",
    "print(f\"  • Ridership coverage: {ridership_coverage:.1f}%\")\n",
    "print(f\"  • Weather coverage:   {weather_coverage:.1f}%\")\n",
    "\n",
    "# Interpret coverage\n",
    "if ridership_coverage >= 95 and weather_coverage >= 95:\n",
    "    print(\"Temporal alignment: EXCELLENT\")\n",
    "else:\n",
    "    print(\"Warning: Overlap is below recommended threshold\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f8aaf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CORE DATASET INTEGRATION\n",
      "============================================================\n",
      "Pre-integration stats:\n",
      "• Ridership records: 1,052,709\n",
      "• Weather records:   8,783\n",
      "\n",
      "Joining on 'transit_timestamp' (left join)...\n",
      "\n",
      "Post-integration stats:\n",
      "• Total records:     1,052,709\n",
      "• Matched weather:   1,052,709 (100.0%)\n",
      "• Missing weather:   0 (0.0%)\n",
      "• Columns in output: 23\n",
      "\n",
      "Integration quality: EXCELLENT\n",
      "Weather coverage is sufficient for feature engineering\n",
      "\n",
      "Sample of integrated data:\n",
      "Columns shown: ['station_complex_id', 'transit_timestamp', 'ridership', 'temp', 'weather_main', 'is_cbd', 'humidity']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_complex_id</th>\n",
       "      <th>transit_timestamp</th>\n",
       "      <th>ridership</th>\n",
       "      <th>temp</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>is_cbd</th>\n",
       "      <th>humidity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>1141</td>\n",
       "      <td>5.75</td>\n",
       "      <td>Rain</td>\n",
       "      <td>1</td>\n",
       "      <td>59.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>2024-01-01 01:00:00</td>\n",
       "      <td>351</td>\n",
       "      <td>5.43</td>\n",
       "      <td>Rain</td>\n",
       "      <td>1</td>\n",
       "      <td>60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>2024-01-01 02:00:00</td>\n",
       "      <td>94</td>\n",
       "      <td>5.45</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>1</td>\n",
       "      <td>62.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_complex_id   transit_timestamp  ridership  temp weather_main  \\\n",
       "0                   8 2024-01-01 00:00:00       1141  5.75         Rain   \n",
       "1                   8 2024-01-01 01:00:00        351  5.43         Rain   \n",
       "2                   8 2024-01-01 02:00:00         94  5.45       Clouds   \n",
       "\n",
       "   is_cbd  humidity  \n",
       "0       1     59.00  \n",
       "1       1     60.00  \n",
       "2       1     62.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================================\n",
    "# Step 4: Core Dataset Integration\n",
    "# =========================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CORE DATASET INTEGRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Pre-integration metrics\n",
    "print(\"Pre-integration stats:\")\n",
    "print(f\"• Ridership records: {len(ridership_df):,}\")\n",
    "print(f\"• Weather records:   {len(weather_df):,}\")\n",
    "\n",
    "# Perform left join on 'transit_timestamp'\n",
    "print(\"\\nJoining on 'transit_timestamp' (left join)...\")\n",
    "integrated_df = ridership_df.merge(\n",
    "    weather_df,\n",
    "    on=\"transit_timestamp\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_weather\")\n",
    ")\n",
    "\n",
    "# Post-integration stats\n",
    "total_records = len(integrated_df)\n",
    "matched_weather = integrated_df['temp'].notna().sum()\n",
    "missing_weather = total_records - matched_weather\n",
    "match_rate = matched_weather / total_records * 100\n",
    "\n",
    "print(\"\\nPost-integration stats:\")\n",
    "print(f\"• Total records:     {total_records:,}\")\n",
    "print(f\"• Matched weather:   {matched_weather:,} ({match_rate:.1f}%)\")\n",
    "print(f\"• Missing weather:   {missing_weather:,} ({100 - match_rate:.1f}%)\")\n",
    "print(f\"• Columns in output: {len(integrated_df.columns)}\")\n",
    "\n",
    "# Integration quality assessment\n",
    "if match_rate >= 98:\n",
    "    quality = \"EXCELLENT\"\n",
    "elif match_rate >= 95:\n",
    "    quality = \"VERY GOOD\"\n",
    "elif match_rate >= 90:\n",
    "    quality = \"GOOD\"\n",
    "else:\n",
    "    quality = \"NEEDS REVIEW\"\n",
    "\n",
    "print(f\"\\nIntegration quality: {quality}\")\n",
    "if match_rate < 95:\n",
    "    print(\"Note: Some ridership records may lack weather data\")\n",
    "else:\n",
    "    print(\"Weather coverage is sufficient for feature engineering\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of integrated data:\")\n",
    "sample_columns = [\n",
    "    \"station_complex_id\", \"transit_timestamp\", \"ridership\",\n",
    "    \"temp\", \"weather_main\", \"is_cbd\", \"humidity\", \"wind_speed\"\n",
    "]\n",
    "sample_columns = [col for col in sample_columns if col in integrated_df.columns][:7]\n",
    "\n",
    "print(f\"Columns shown: {sample_columns}\")\n",
    "display(integrated_df[sample_columns].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8c59bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BASIC DATASET VALIDATION\n",
      "============================================================\n",
      "Validating structure and completeness of integrated dataset...\n",
      "\n",
      "Integrated Dataset Overview:\n",
      "• Total records:       1,052,709\n",
      "• Unique stations:     121\n",
      "• Unique timestamps:   8,783\n",
      "• Date range:          2024-01-01 00:00:00 to 2024-12-31 23:00:00\n",
      "• Total columns:       23\n",
      "\n",
      "Basic Quality Checks:\n",
      "• Duplicate rows (station + time): 0\n",
      "• Records missing weather info:    0\n",
      "• Integration status:              EXCELLENT\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Step 5: Basic Dataset Validation\n",
    "# =========================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASIC DATASET VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"Validating structure and completeness of integrated dataset...\\n\")\n",
    "\n",
    "# Dataset summary\n",
    "print(\"Integrated Dataset Overview:\")\n",
    "print(f\"• Total records:       {len(integrated_df):,}\")\n",
    "print(f\"• Unique stations:     {integrated_df['station_complex_id'].nunique()}\")\n",
    "print(f\"• Unique timestamps:   {integrated_df['transit_timestamp'].nunique():,}\")\n",
    "print(f\"• Date range:          {integrated_df['transit_timestamp'].min()} to {integrated_df['transit_timestamp'].max()}\")\n",
    "print(f\"• Total columns:       {len(integrated_df.columns)}\")\n",
    "\n",
    "# Quality checks\n",
    "print(\"\\nBasic Quality Checks:\")\n",
    "\n",
    "# Duplicate detection\n",
    "duplicates = integrated_df.duplicated(subset=[\"station_complex_id\", \"transit_timestamp\"]).sum()\n",
    "print(f\"• Duplicate rows (station + time): {duplicates}\")\n",
    "\n",
    "# Weather data coverage\n",
    "missing_weather = integrated_df['temp'].isna().sum()\n",
    "print(f\"• Records missing weather info:    {missing_weather}\")\n",
    "\n",
    "# Validation result\n",
    "if duplicates == 0 and missing_weather == 0:\n",
    "    print(\"• Integration status:              EXCELLENT\")\n",
    "elif duplicates == 0 and missing_weather <= 0.05 * len(integrated_df):\n",
    "    print(\"• Integration status:              GOOD (minor missing weather)\")\n",
    "else:\n",
    "    print(\"• Integration status:              NEEDS REVIEW\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac19a9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAVING INTEGRATED DATASET\n",
      "============================================================\n",
      "Integrated dataset saved: weather_ridership_integrated_2024.parquet\n",
      "Location: C:\\Users\\neasa\\manhattan-subway\\data\\processed\\integration\\weather_ridership_integrated_2024.parquet\n",
      "File size: 10.3 MB\n",
      "Total records: 1,052,709\n",
      "Total columns: 23\n",
      "\n",
      "============================================================\n",
      "INTEGRATION COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "Next steps:\n",
      "1. Temporal pattern analysis (Notebook 04)\n",
      "2. Weather-ridership correlation analysis (Notebook 05)\n",
      "3. Feature engineering for modeling (Notebook 06)\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Step 6: Save Integrated Dataset\n",
    "# =========================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAVING INTEGRATED DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Export dataset using predefined OUTPUT_FILE\n",
    "integrated_df.to_parquet(OUTPUT_FILE, index=False)\n",
    "\n",
    "# Report output summary\n",
    "file_size_mb = OUTPUT_FILE.stat().st_size / (1024 * 1024)\n",
    "print(f\"Integrated dataset saved: {OUTPUT_FILE.name}\")\n",
    "print(f\"Location: {OUTPUT_FILE.resolve()}\")\n",
    "print(f\"File size: {file_size_mb:.1f} MB\")\n",
    "print(f\"Total records: {len(integrated_df):,}\")\n",
    "print(f\"Total columns: {len(integrated_df.columns)}\")\n",
    "\n",
    "# Final status message\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INTEGRATION COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Next steps:\")\n",
    "print(\"1. Temporal pattern analysis (Notebook 04)\")\n",
    "print(\"2. Weather-ridership correlation analysis (Notebook 05)\")\n",
    "print(\"3. Feature engineering for modeling (Notebook 06)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manhattan-subway",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
