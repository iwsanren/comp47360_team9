{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a528196",
   "metadata": {},
   "source": [
    "# 07 - Manhattan Subway Ridership Model Development  \n",
    "Analysis-Driven Machine Learning Pipeline with Business Logic Validation\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "**Objective:**  \n",
    "Develop production-ready machine learning models that capture and validate discovered ridership behavior patterns.\n",
    "\n",
    "**Input Dataset:**  \n",
    "- `subway_ridership_modeling_features.parquet`  \n",
    "- 24 validated features from prior feature engineering phase\n",
    "\n",
    "**Output Artifacts:**  \n",
    "- Trained and validated model object (e.g. XGBoost)\n",
    "- Evaluation metrics and comparison summary\n",
    "- Feature importance analysis\n",
    "- Business logic validation of key model outputs\n",
    "\n",
    "**Modeling Focus Areas:**\n",
    "- Pattern fidelity (rush hour, weekend, weather responsiveness)\n",
    "- Predictive accuracy across time (Jan–Oct train / Nov–Dec test)\n",
    "- Interpretability and deployment readiness\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f056f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "NOTEBOOK 07: MANHATTAN SUBWAY MODEL DEVELOPMENT\n",
      "============================================================\n",
      "Analysis Date: 2025-07-28 20:31:10\n",
      "Methodology: Model Comparison + XGBoost Optimization\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Setup and Configuration\n",
    "# =============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core ML\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Optional Boosting Models\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# NOTE: np.bool_, np.integer, np.floating will need conversion before JSON serialization\n",
    "# See: convert_to_json_serializable() helper later in the pipeline\n",
    "\n",
    "# Notebook header\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NOTEBOOK 07: MANHATTAN SUBWAY MODEL DEVELOPMENT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"Methodology: Model Comparison + XGBoost Optimization\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce394a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 1. Data Loading and Temporal Splitting\n",
    "# =============================================\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load dataset and separate features, target, and identifiers.\"\"\"\n",
    "    print(\"\\n1. DATA LOADING AND PREPARATION\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Look for dataset\n",
    "    possible_paths = [\n",
    "        \"../data/processed/modeling/subway_ridership_modeling_features.parquet\",\n",
    "        \"data/processed/modeling/subway_ridership_modeling_features.parquet\"\n",
    "    ]\n",
    "\n",
    "    dataset_path = next((Path(p) for p in possible_paths if Path(p).exists()), None)\n",
    "    if not dataset_path:\n",
    "        raise FileNotFoundError(f\"Dataset not found in any of the following paths:\\n  \" + \"\\n  \".join(possible_paths))\n",
    "\n",
    "    print(f\"Loading dataset from: {dataset_path}\")\n",
    "    df = pd.read_parquet(dataset_path)\n",
    "\n",
    "    # Define columns\n",
    "    identifier_cols = ['station_complex_id', 'station_complex', 'transit_timestamp']\n",
    "    target_col = 'ridership'\n",
    "    feature_cols = [col for col in df.columns if col not in identifier_cols + [target_col]]\n",
    "\n",
    "    # Extract components\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target_col].copy()\n",
    "    identifiers = df[identifier_cols].copy()\n",
    "\n",
    "    print(f\"Dataset loaded:     {df.shape}\")\n",
    "    print(f\"Features:           {len(feature_cols)}\")\n",
    "    print(f\"Target column:      {target_col}\")\n",
    "    print(f\"Date range:         {df['transit_timestamp'].min()} → {df['transit_timestamp'].max()}\")\n",
    "\n",
    "    return X, y, identifiers, feature_cols\n",
    "\n",
    "\n",
    "def create_temporal_splits(X, y, identifiers):\n",
    "    \"\"\"Create train/test split based on timestamp cutoff (Nov–Dec holdout).\"\"\"\n",
    "    print(\"\\n2. TEMPORAL SPLITTING\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    timestamps = identifiers['transit_timestamp']\n",
    "    if not pd.api.types.is_datetime64_any_dtype(timestamps):\n",
    "        timestamps = pd.to_datetime(timestamps)\n",
    "\n",
    "    sort_idx = timestamps.argsort()\n",
    "\n",
    "    X_sorted = X.iloc[sort_idx].reset_index(drop=True)\n",
    "    y_sorted = y.iloc[sort_idx].reset_index(drop=True)\n",
    "    ts_sorted = timestamps.iloc[sort_idx].reset_index(drop=True)\n",
    "\n",
    "    train_cutoff = pd.Timestamp(\"2024-10-31 23:59:59\")\n",
    "    train_mask = ts_sorted <= train_cutoff\n",
    "\n",
    "    X_train = X_sorted[train_mask].reset_index(drop=True)\n",
    "    X_test = X_sorted[~train_mask].reset_index(drop=True)\n",
    "    y_train = y_sorted[train_mask].reset_index(drop=True)\n",
    "    y_test = y_sorted[~train_mask].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Train samples:      {len(X_train):,}\")\n",
    "    print(f\"Test samples:       {len(X_test):,}\")\n",
    "    print(f\"Train target mean:  {y_train.mean():.0f}\")\n",
    "    print(f\"Test target mean:   {y_test.mean():.0f}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ed4faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 3. Model Definition, Evaluation, and Comparison\n",
    "# =============================================\n",
    "\n",
    "def define_models():\n",
    "    \"\"\"Define baseline models.\"\"\"\n",
    "    models = {\n",
    "        'LinearRegression': LinearRegression(),\n",
    "        'Ridge': Ridge(alpha=1.0, random_state=42),\n",
    "        'Lasso': Lasso(alpha=1.0, random_state=42, max_iter=2000),\n",
    "        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    }\n",
    "\n",
    "    if XGBOOST_AVAILABLE:\n",
    "        models['XGBoost'] = xgb.XGBRegressor(n_estimators=100, random_state=42, verbosity=0)\n",
    "\n",
    "    if LIGHTGBM_AVAILABLE:\n",
    "        models['LightGBM'] = lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "def evaluate_model(name, model, X_train, X_test, y_train, y_test, skip_cv=False):\n",
    "    \"\"\"Evaluate a single model with optional cross-validation.\"\"\"\n",
    "    \n",
    "    # Pipeline (scaling for linear models only)\n",
    "    if name in ['LinearRegression', 'Ridge', 'Lasso']:\n",
    "        pipeline = Pipeline([('scaler', StandardScaler()), ('model', model)])\n",
    "    else:\n",
    "        pipeline = Pipeline([('model', model)])\n",
    "\n",
    "    # Cross-validation\n",
    "    if not skip_cv:\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        cv_scores = cross_val_score(\n",
    "            pipeline, X_train, y_train, cv=tscv,\n",
    "            scoring='neg_root_mean_squared_error', n_jobs=-1\n",
    "        )\n",
    "        cv_rmse = -cv_scores.mean()\n",
    "        cv_std = cv_scores.std()\n",
    "    else:\n",
    "        cv_rmse, cv_std = 0, 0\n",
    "\n",
    "    # Training\n",
    "    start_time = datetime.now()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    training_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "    # Predictions\n",
    "    train_pred = pipeline.predict(X_train)\n",
    "    test_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Evaluation\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "    train_r2 = r2_score(y_train, train_pred)\n",
    "    test_r2 = r2_score(y_test, test_pred)\n",
    "\n",
    "    rmse_gap_pct = ((test_rmse - train_rmse) / test_rmse) * 100\n",
    "    r2_gap = train_r2 - test_r2\n",
    "\n",
    "    return {\n",
    "        'model': pipeline,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'rmse_gap_pct': rmse_gap_pct,\n",
    "        'r2_gap': r2_gap,\n",
    "        'cv_rmse': cv_rmse,\n",
    "        'cv_std': cv_std,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "\n",
    "\n",
    "def compare_models(models, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Compare all baseline models.\"\"\"\n",
    "    print(\"\\n3. MODEL COMPARISON\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    results = {}\n",
    "    comparison_data = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        result = evaluate_model(name, model, X_train, X_test, y_train, y_test)\n",
    "        results[name] = result\n",
    "\n",
    "        comparison_data.append({\n",
    "            'Model': name,\n",
    "            'Train_RMSE': result['train_rmse'],\n",
    "            'Test_RMSE': result['test_rmse'],\n",
    "            'Train_R2': result['train_r2'],\n",
    "            'Test_R2': result['test_r2'],\n",
    "            'RMSE_Gap_%': result['rmse_gap_pct'],\n",
    "            'R2_Gap': result['r2_gap'],\n",
    "            'CV_RMSE': result['cv_rmse']\n",
    "        })\n",
    "\n",
    "        print(f\"  RMSE: {result['test_rmse']:.0f}, R²: {result['test_r2']:.3f}, RMSE Gap: {result['rmse_gap_pct']:.1f}%, R² Gap: {result['r2_gap']:.3f}\")\n",
    "\n",
    "    # Tabular summary\n",
    "    df_comparison = pd.DataFrame(comparison_data).sort_values('Test_R2', ascending=False)\n",
    "\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(df_comparison.round({\n",
    "        'Train_RMSE': 0, 'Test_RMSE': 0,\n",
    "        'Train_R2': 3, 'Test_R2': 3,\n",
    "        'RMSE_Gap_%': 1, 'R2_Gap': 3, 'CV_RMSE': 0\n",
    "    }).to_string(index=False))\n",
    "\n",
    "    return results, df_comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3782c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 4. XGBoost Optimization and Evaluation\n",
    "# =============================================\n",
    "\n",
    "def optimize_xgboost(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Train and evaluate XGBoost with tuned hyperparameters.\"\"\"\n",
    "    print(\"\\n4. XGBOOST OPTIMIZATION\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Tuned hyperparameters\n",
    "    xgb_params = {\n",
    "        'n_estimators': 1000,\n",
    "        'max_depth': 4,\n",
    "        'reg_alpha': 0.3,\n",
    "        'reg_lambda': 0.3,\n",
    "        'min_child_weight': 8,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'learning_rate': 0.05,\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    print(\"Model Parameters:\")\n",
    "    for k, v in xgb_params.items():\n",
    "        print(f\"  {k:<20}: {v}\")\n",
    "\n",
    "    # Train model\n",
    "    print(\"\\nTraining XGBoost...\")\n",
    "    model = xgb.XGBRegressor(**xgb_params)\n",
    "    start_time = datetime.now()\n",
    "    model.fit(X_train, y_train)  # Optional: add early_stopping_rounds, eval_set\n",
    "    training_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "    # Predict\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "    train_r2 = r2_score(y_train, train_pred)\n",
    "    test_r2 = r2_score(y_test, test_pred)\n",
    "    rmse_gap_pct = ((test_rmse - train_rmse) / test_rmse) * 100\n",
    "    r2_gap = train_r2 - test_r2\n",
    "\n",
    "    # Cross-validation\n",
    "    print(\"\\nCross-validation (TimeSeriesSplit)...\")\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train, y_train,\n",
    "        cv=tscv, scoring='neg_root_mean_squared_error', n_jobs=-1\n",
    "    )\n",
    "    cv_rmse_scores = -cv_scores\n",
    "    cv_rmse_mean = cv_rmse_scores.mean()\n",
    "    cv_rmse_std = cv_rmse_scores.std()\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n--- Evaluation Results ---\")\n",
    "    print(f\"Train RMSE:      {train_rmse:.2f}\")\n",
    "    print(f\"Test RMSE:       {test_rmse:.2f}\")\n",
    "    print(f\"Train R²:        {train_r2:.3f}\")\n",
    "    print(f\"Test R²:         {test_r2:.3f}\")\n",
    "    print(f\"RMSE Gap (%):    {rmse_gap_pct:.1f}%\")\n",
    "    print(f\"R² Gap:          {r2_gap:.3f}\")\n",
    "    print(f\"Training Time:   {training_time:.1f} sec\")\n",
    "\n",
    "    print(\"\\nCross-Validation RMSE Scores:\")\n",
    "    for i, score in enumerate(cv_rmse_scores, 1):\n",
    "        print(f\"  Fold {i}: {score:.2f}\")\n",
    "    print(f\"Mean CV RMSE:    {cv_rmse_mean:.2f}\")\n",
    "    print(f\"CV RMSE Std:     {cv_rmse_std:.2f}\")\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'params': xgb_params,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'rmse_gap_pct': rmse_gap_pct,\n",
    "        'r2_gap': r2_gap,\n",
    "        'cv_rmse_mean': cv_rmse_mean,\n",
    "        'cv_rmse_std': cv_rmse_std,\n",
    "        'cv_scores': cv_rmse_scores,\n",
    "        'training_time': training_time,\n",
    "        'trees_used': xgb_params['n_estimators']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7a6e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 5. Feature Importance Analysis\n",
    "# =============================================\n",
    "\n",
    "def analyze_feature_importance(model_result, feature_names):\n",
    "    \"\"\"Analyze and display feature importances from the optimized XGBoost model.\"\"\"\n",
    "    print(\"\\n5. FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    model = model_result['model']\n",
    "\n",
    "    if not hasattr(model, 'feature_importances_'):\n",
    "        print(\"Feature importance not available for this model type.\")\n",
    "        return None\n",
    "\n",
    "    importances = model.feature_importances_\n",
    "\n",
    "    if len(importances) != len(feature_names):\n",
    "        print(\"Warning: Feature importances do not match number of features.\")\n",
    "        return None\n",
    "\n",
    "    # Create importance DataFrame\n",
    "    importance_df = (\n",
    "        pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "        .sort_values('importance', ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(f\"Feature Importance Rankings ({len(feature_names)} features):\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, row in importance_df.iterrows():\n",
    "        print(f\"  {i+1:2d}. {row['feature']:<20}: {row['importance']:.5f}\")\n",
    "\n",
    "    # Category breakdown\n",
    "    temporal_features = [\n",
    "        'hour', 'day_of_week', 'month', 'hour_sin', 'hour_cos',\n",
    "        'dow_sin', 'dow_cos', 'month_sin', 'month_cos',\n",
    "        'is_rush_hour', 'is_weekend', 'is_holiday'\n",
    "    ]\n",
    "    weather_features = [\n",
    "        'temp', 'humidity', 'wind_speed', 'feels_like',\n",
    "        'has_snow', 'has_rain', 'is_freezing', 'is_hot', 'temp_category'\n",
    "    ]\n",
    "    location_features = ['latitude', 'longitude', 'is_cbd']\n",
    "\n",
    "    temporal_imp = importance_df[importance_df['feature'].isin(temporal_features)]['importance'].sum()\n",
    "    weather_imp = importance_df[importance_df['feature'].isin(weather_features)]['importance'].sum()\n",
    "    location_imp = importance_df[importance_df['feature'].isin(location_features)]['importance'].sum()\n",
    "\n",
    "    print(\"\\nImportance by Feature Category:\")\n",
    "    print(f\"  Temporal Features : {temporal_imp:.5f}\")\n",
    "    print(f\"  Weather Features  : {weather_imp:.5f}\")\n",
    "    print(f\"  Location Features : {location_imp:.5f}\")\n",
    "\n",
    "    return importance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26d4b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 6. Business Logic Validation (Predicted Behavior)\n",
    "# =============================================\n",
    "\n",
    "def validate_business_logic(model_result, X_test):\n",
    "    \"\"\"Validate whether model predictions align with known ridership patterns.\"\"\"\n",
    "    print(\"\\n6. BUSINESS LOGIC VALIDATION\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    model = model_result['model']\n",
    "    test_pred = model.predict(X_test)\n",
    "\n",
    "    test_data = X_test.copy()\n",
    "    test_data['predicted'] = test_pred\n",
    "\n",
    "    validation_results = {}\n",
    "\n",
    "    # Rush hour pattern\n",
    "    if 'is_rush_hour' in test_data.columns:\n",
    "        rush_avg = test_data[test_data['is_rush_hour'] == 1]['predicted'].mean()\n",
    "        non_rush_avg = test_data[test_data['is_rush_hour'] == 0]['predicted'].mean()\n",
    "        rush_factor = rush_avg / non_rush_avg if non_rush_avg > 0 else np.nan\n",
    "        validation_results['rush_hour_factor'] = rush_factor\n",
    "        print(f\"Rush hour factor:    {rush_factor:.2f}x (expected ~2.36x)\")\n",
    "\n",
    "    # Weekend pattern\n",
    "    if 'is_weekend' in test_data.columns:\n",
    "        weekend_avg = test_data[test_data['is_weekend'] == 1]['predicted'].mean()\n",
    "        weekday_avg = test_data[test_data['is_weekend'] == 0]['predicted'].mean()\n",
    "        weekend_factor = weekend_avg / weekday_avg if weekday_avg > 0 else np.nan\n",
    "        validation_results['weekend_factor'] = weekend_factor\n",
    "        print(f\"Weekend factor:      {weekend_factor:.2f}x (expected ~0.63x)\")\n",
    "\n",
    "    # CBD vs non-CBD pattern\n",
    "    if 'is_cbd' in test_data.columns:\n",
    "        cbd_avg = test_data[test_data['is_cbd'] == 1]['predicted'].mean()\n",
    "        non_cbd_avg = test_data[test_data['is_cbd'] == 0]['predicted'].mean()\n",
    "        cbd_factor = cbd_avg / non_cbd_avg if non_cbd_avg > 0 else np.nan\n",
    "        validation_results['cbd_factor'] = cbd_factor\n",
    "        print(f\"CBD location factor: {cbd_factor:.2f}x (expected ~2.52x)\")\n",
    "\n",
    "    # Pattern validation status flags\n",
    "    validation_results['status'] = {\n",
    "        'rush_hour_valid': 2.0 <= validation_results.get('rush_hour_factor', 0) <= 3.0,\n",
    "        'weekend_valid':   0.55 <= validation_results.get('weekend_factor', 1) <= 0.75,\n",
    "        'cbd_valid':       2.0 <= validation_results.get('cbd_factor', 0) <= 3.5\n",
    "    }\n",
    "\n",
    "    return validation_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47dc0cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 7. Save Final Model and Metadata\n",
    "# =============================================\n",
    "\n",
    "def save_final_model(model_result, feature_names, importance_df, validation_results):\n",
    "    \"\"\"Save the final trained model along with metadata and supporting files.\"\"\"\n",
    "    print(\"\\n7. SAVING FINAL MODEL\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    output_dir = Path(\"../data/processed/models\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save model\n",
    "    model_file = output_dir / \"subway_ridership_model_xgboost_final.joblib\"\n",
    "    joblib.dump(model_result['model'], model_file)\n",
    "\n",
    "    # Helper for JSON serialization\n",
    "    def convert_to_json_serializable(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.bool_):  \n",
    "            return bool(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: convert_to_json_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_to_json_serializable(i) for i in obj]\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    # Build metadata\n",
    "    metadata = {\n",
    "        'model_name': 'XGBoost_Optimized_Final',\n",
    "        'creation_date': datetime.now().isoformat(),\n",
    "        'model_file': str(model_file.name),\n",
    "        'loading_instructions': \"Use joblib.load() to load the model\",\n",
    "        'performance': {\n",
    "            'test_rmse': float(model_result['test_rmse']),\n",
    "            'test_r2': float(model_result['test_r2']),\n",
    "            'train_r2': float(model_result.get('train_r2', 0)),\n",
    "            'rmse_gap_pct': float(model_result['rmse_gap_pct']),\n",
    "            'r2_gap': float(model_result.get('r2_gap', 0)),\n",
    "            'cv_rmse_mean': float(model_result['cv_rmse_mean']),\n",
    "            'cv_rmse_std': float(model_result['cv_rmse_std']),\n",
    "            'training_time_sec': float(model_result['training_time']),\n",
    "            'trees_used': int(model_result['trees_used'])\n",
    "        },\n",
    "        'cv_scores': convert_to_json_serializable(model_result['cv_scores']),\n",
    "        'hyperparameters': convert_to_json_serializable(model_result['params']),\n",
    "        'features': feature_names,\n",
    "        'business_validation': convert_to_json_serializable(validation_results),\n",
    "        'feature_importance_top10': (\n",
    "            importance_df.head(10).to_dict('records') if importance_df is not None else None\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Save metadata\n",
    "    metadata_file = output_dir / \"model_metadata_xgboost_final.json\"\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    # Save feature list separately\n",
    "    features_file = output_dir / \"required_features.json\"\n",
    "    with open(features_file, 'w') as f:\n",
    "        json.dump(feature_names, f, indent=2)\n",
    "\n",
    "    print(f\"Model saved:     {model_file}\")\n",
    "    print(f\"Metadata saved:  {metadata_file}\")\n",
    "    print(f\"Feature list:    {features_file}\")\n",
    "\n",
    "    return [model_file, metadata_file, features_file]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e13dce92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING PIPELINE\n",
      "============================================================\n",
      "\n",
      "1. DATA LOADING AND PREPARATION\n",
      "----------------------------------------\n",
      "Loading dataset from: ..\\data\\processed\\modeling\\subway_ridership_modeling_features.parquet\n",
      "Dataset loaded:     (1052709, 28)\n",
      "Features:           24\n",
      "Target column:      ridership\n",
      "Date range:         2024-01-01 00:00:00 → 2024-12-31 23:00:00\n",
      "\n",
      "2. TEMPORAL SPLITTING\n",
      "----------------------------------------\n",
      "Train samples:      876,990\n",
      "Test samples:       175,719\n",
      "Train target mean:  637\n",
      "Test target mean:   674\n",
      "\n",
      "3. MODEL COMPARISON\n",
      "----------------------------------------\n",
      "Training LinearRegression...\n",
      "  RMSE: 1054, R²: 0.206, RMSE Gap: 5.9%, R² Gap: 0.021\n",
      "Training Ridge...\n",
      "  RMSE: 1054, R²: 0.206, RMSE Gap: 5.9%, R² Gap: 0.021\n",
      "Training Lasso...\n",
      "  RMSE: 1048, R²: 0.215, RMSE Gap: 5.3%, R² Gap: 0.012\n",
      "Training RandomForest...\n",
      "  RMSE: 316, R²: 0.929, RMSE Gap: 83.7%, R² Gap: 0.069\n",
      "Training XGBoost...\n",
      "  RMSE: 354, R²: 0.910, RMSE Gap: 43.9%, R² Gap: 0.059\n",
      "Training LightGBM...\n",
      "  RMSE: 375, R²: 0.899, RMSE Gap: 34.3%, R² Gap: 0.053\n",
      "\n",
      "Model Comparison:\n",
      "           Model  Train_RMSE  Test_RMSE  Train_R2  Test_R2  RMSE_Gap_%  R2_Gap  CV_RMSE\n",
      "    RandomForest        52.0      316.0     0.998    0.929        83.7   0.069    188.0\n",
      "         XGBoost       198.0      354.0     0.969    0.910        43.9   0.059    236.0\n",
      "        LightGBM       247.0      375.0     0.952    0.899        34.3   0.053    265.0\n",
      "           Lasso       992.0     1048.0     0.228    0.215         5.3   0.012   1004.0\n",
      "           Ridge       992.0     1054.0     0.228    0.206         5.9   0.021   1004.0\n",
      "LinearRegression       992.0     1054.0     0.228    0.206         5.9   0.021   1004.0\n",
      "\n",
      "4. XGBOOST OPTIMIZATION\n",
      "----------------------------------------\n",
      "Model Parameters:\n",
      "  n_estimators        : 1000\n",
      "  max_depth           : 4\n",
      "  reg_alpha           : 0.3\n",
      "  reg_lambda          : 0.3\n",
      "  min_child_weight    : 8\n",
      "  subsample           : 0.8\n",
      "  colsample_bytree    : 0.8\n",
      "  learning_rate       : 0.05\n",
      "  random_state        : 42\n",
      "  verbosity           : 0\n",
      "  n_jobs              : -1\n",
      "\n",
      "Training XGBoost...\n",
      "\n",
      "Cross-validation (TimeSeriesSplit)...\n",
      "\n",
      "--- Evaluation Results ---\n",
      "Train RMSE:      305.42\n",
      "Test RMSE:       403.66\n",
      "Train R²:        0.927\n",
      "Test R²:         0.884\n",
      "RMSE Gap (%):    24.3%\n",
      "R² Gap:          0.043\n",
      "Training Time:   40.2 sec\n",
      "\n",
      "Cross-Validation RMSE Scores:\n",
      "  Fold 1: 335.36\n",
      "  Fold 2: 302.49\n",
      "  Fold 3: 310.85\n",
      "  Fold 4: 276.60\n",
      "  Fold 5: 368.30\n",
      "Mean CV RMSE:    318.72\n",
      "CV RMSE Std:     31.09\n",
      "\n",
      "5. FEATURE IMPORTANCE ANALYSIS\n",
      "----------------------------------------\n",
      "Feature Importance Rankings (24 features):\n",
      "--------------------------------------------------\n",
      "   1. is_cbd              : 0.20834\n",
      "   2. latitude            : 0.14392\n",
      "   3. hour_sin            : 0.11166\n",
      "   4. hour_cos            : 0.09608\n",
      "   5. is_weekend          : 0.09460\n",
      "   6. hour                : 0.08133\n",
      "   7. is_rush_hour        : 0.07076\n",
      "   8. longitude           : 0.05979\n",
      "   9. day_of_week         : 0.05371\n",
      "  10. dow_sin             : 0.04718\n",
      "  11. is_holiday          : 0.01179\n",
      "  12. month               : 0.00693\n",
      "  13. dow_cos             : 0.00457\n",
      "  14. month_cos           : 0.00171\n",
      "  15. temp                : 0.00168\n",
      "  16. humidity            : 0.00159\n",
      "  17. has_rain            : 0.00138\n",
      "  18. wind_speed          : 0.00089\n",
      "  19. feels_like          : 0.00087\n",
      "  20. month_sin           : 0.00067\n",
      "  21. has_snow            : 0.00056\n",
      "  22. is_hot              : 0.00000\n",
      "  23. temp_category       : 0.00000\n",
      "  24. is_freezing         : 0.00000\n",
      "\n",
      "Importance by Feature Category:\n",
      "  Temporal Features : 0.58099\n",
      "  Weather Features  : 0.00696\n",
      "  Location Features : 0.41205\n",
      "\n",
      "6. BUSINESS LOGIC VALIDATION\n",
      "----------------------------------------\n",
      "Rush hour factor:    2.32x (expected ~2.36x)\n",
      "Weekend factor:      0.64x (expected ~0.63x)\n",
      "CBD location factor: 2.40x (expected ~2.52x)\n",
      "\n",
      "7. SAVING FINAL MODEL\n",
      "----------------------------------------\n",
      "Model saved:     ..\\data\\processed\\models\\subway_ridership_model_xgboost_final.joblib\n",
      "Metadata saved:  ..\\data\\processed\\models\\model_metadata_xgboost_final.json\n",
      "Feature list:    ..\\data\\processed\\models\\required_features.json\n",
      "\n",
      "============================================================\n",
      "PIPELINE COMPLETED\n",
      "============================================================\n",
      "Final Model:         RMSE        = 404\n",
      "                     R²          = 0.884\n",
      "                     RMSE Gap %  = 24.3%\n",
      "                     R² Gap      = 0.043\n",
      "Files saved:         3\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# 8. Pipeline Orchestration\n",
    "# =============================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the complete modeling pipeline end-to-end.\"\"\"\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STARTING PIPELINE\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # 1. Data preparation\n",
    "        X, y, identifiers, feature_names = load_and_prepare_data()\n",
    "        X_train, X_test, y_train, y_test = create_temporal_splits(X, y, identifiers)\n",
    "\n",
    "        # 2. Model comparison\n",
    "        models = define_models()\n",
    "        baseline_results, baseline_comparison = compare_models(models, X_train, X_test, y_train, y_test)\n",
    "\n",
    "        # 3. Optimized XGBoost model\n",
    "        optimized_model_result = optimize_xgboost(X_train, X_test, y_train, y_test)\n",
    "\n",
    "        # 4. Feature importance analysis\n",
    "        importance_df = analyze_feature_importance(optimized_model_result, feature_names)\n",
    "\n",
    "        # 5. Business logic validation\n",
    "        validation_results = validate_business_logic(optimized_model_result, X_test)\n",
    "\n",
    "        # 6. Save model + metadata\n",
    "        saved_files = save_final_model(optimized_model_result, feature_names, importance_df, validation_results)\n",
    "\n",
    "        # 7. Final summary\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"PIPELINE COMPLETED\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Final Model:         RMSE        = {optimized_model_result['test_rmse']:.0f}\")\n",
    "        print(f\"                     R²          = {optimized_model_result['test_r2']:.3f}\")\n",
    "        print(f\"                     RMSE Gap %  = {optimized_model_result['rmse_gap_pct']:.1f}%\")\n",
    "        print(f\"                     R² Gap      = {optimized_model_result['r2_gap']:.3f}\")\n",
    "        print(f\"Files saved:         {len(saved_files)}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        return {\n",
    "            'baseline_results': baseline_results,\n",
    "            'optimized_model': optimized_model_result,\n",
    "            'importance_df': importance_df,\n",
    "            'validation': validation_results,\n",
    "            'files': saved_files\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\nERROR OCCURRED DURING PIPELINE EXECUTION\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"{e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Execute pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()\n",
    "else:\n",
    "    results = main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manhattan-subway",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
